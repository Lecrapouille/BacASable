# ChatGPT

Test the C++ code generated by ChatGPT https://chat.openai.com/

Dependencies for compiling:
- https://github.com/nholthaus/units
- https://github.com/SFML/SFML

To compile files, see the comment of the `main()` function, it gives the command
for compiling the file.

My requests for chatGPT are described in the header comment (in French) of each
files. I also described issues in the code generated and what I did to fix them.

Edit: thanks to https://github.com/ryanschiang/chatgpt-export you can export
your talks to markdown. I added my talks concerning [02_road.md](02_road.md).

My conclusion:
- The comprehension of natural language by ChatGPT is very impressive.
- ChatGPT understands what I want. The first try (first chat) was always
  impressive, but starting a new chat and asking for the same request with small
  changes makes the code totally different and less good. I thought that a very
  specific request would produce a unique answer (or small variation), but that
  was not the case.
- The generated C++ code contains a lot of comments, which is appreciated.
- The generated C++ code looks good at first read, but it contains errors. Therefore, the code cannot be trusted at first and must be refactored.
- These errors are similar to my observation concerning ChatGPT and mathematics:
  ChatGPT has good knowledge, but the computations have issues. For example: 2
  sugars et 4 chocolate cost 20F. 7 sugars et 8 chocolates cost 52F. ChatGPT
  understands that it has to solve 2x + 4y = 20; 7x + 8y = 52, knows how to
  solve it, but the computations are erroneous.
- It is impressive that ChatGPT was able to understand inferences by itself just
  by reading pages. I guess coupled with a real inference engine (like Prolog)
  or a formal engine (like Mathematica), the power of ChatGPT would be doubled.
